# ğŸšš Real-Time Data Pipeline Using Kafka, Spark & AWS

### For a Logistics Company Operating Between Konstanz and Stuttgart, Germany

This project implements a **real-time data pipeline** using **Apache Kafka**, **Apache Spark**, and **AWS Cloud Services**. It simulates vehicle telemetry data (e.g., GPS coordinates, speed, timestamps) for a logistics company operating in Germany. The data flows through Kafka for ingestion, Spark for processing, and AWS for scalable storage, analytics, and visualization.

---

## ğŸ’» Operating System

This project is developed and tested on:

* **Linux Ubuntu 24.04 LTS**

> âœ… All commands and installation steps are tailored for **Ubuntu 24.04** users.

---

## ğŸ§½ Data Pipeline Architecture

![Data Pipeline Diagram](data_pipeline.png)

**Flow Overview:**

1. **Vehicle Data Producer** (Python) simulates real-time telemetry
2. **Kafka** ingests and streams the data
3. **Spark Structured Streaming** processes the data
4. **AWS S3** stores raw and processed data
5. **AWS Redshift** performs advanced analytics
6. **AWS QuickSight** provides dashboards and visualizations

---

## ğŸ”§ Technologies Used

* **Apache Kafka** â€“ Real-time data streaming
* **Apache Spark** â€“ Stream processing and analytics
* **AWS S3** â€“ Cloud storage for ingested and processed data
* **AWS Redshift** â€“ Scalable data warehousing
* **AWS QuickSight** â€“ Data visualization and reporting
* **Docker** â€“ Containerization of Kafka & Zookeeper
* **Python** â€“ Data simulation and custom scripts

---

## ğŸ› ï¸ Prerequisites

Ensure the following are installed and configured:

* [Docker](https://docs.docker.com/engine/install/ubuntu/)
* Docker Compose *(optional, for easier orchestration)*
* Apache Kafka CLI *(usually included with Kafka Docker images)*
* [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html)
* Java (JDK 8 or above) â€“ Required by Spark
* Python 3.x â€“ For running the data simulation scripts

---

## ğŸ³ Step 1: Access Kafka Broker via Docker

Start your Kafka container, then run the following to interact with it:

```bash
# Access the Kafka broker container
docker exec -it kafka-broker bash

# List available topics
kafka-topics --bootstrap-server localhost:9092 --list

# Read from a specific topic (e.g., vehicle_data)
kafka-console-consumer --bootstrap-server localhost:9092 --topic vehicle_data --from-beginning
```

---

## ğŸ—‚ï¸ Kafka Brokers Overview

Below is a snapshot of the active Kafka brokers configured in the project:

![Kafka Brokers](Screenshot%20from%202025-05-31%2009-25-48.png)

---
## ğŸš€ Data streams in my vehicle_data topic will be like
```bash
kafka-console-consumer --bootstrap-server localhost:9092 --topic vehicle_data --from-beginning

![alt text](<Screenshot from 2025-05-31 09-45-22.png>)
## ğŸš€ Next Steps



---


---

## ğŸš€ Next Steps



---

## ğŸ“ Repository Structure (optional suggestion)

```bash
ğŸ”½ docker/
â”‚   â”œâ”€â”€ kafka-compose.yml
ğŸ”½ data_simulator/
â”‚   â”œâ”€â”€ producer.py
ğŸ”½ spark_jobs/
â”‚   â”œâ”€â”€ spark_streaming.py
ğŸ”½ aws/
â”‚   â”œâ”€â”€ s3_config/
â”‚   â””â”€â”€ redshift_setup.sql
ğŸ”½ README.md
```

---

## ğŸ™Œ Contributions & License

This project is open for contributions. Feel free to fork and improve.

Licensed under [MIT License](LICENSE).
